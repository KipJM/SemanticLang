{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import time\n",
    "\n",
    "import graph_gen_integration\n",
    "from GraphGen.graph_gen_integration import draw_graph\n",
    "\n",
    "graph_gen = graph_gen_integration.GraphGen([], keyword_reward=1.1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "graph_gen.reset_model([], 0)\n",
    "\n",
    "start_time = time.process_time()\n",
    "print(start_time)\n",
    "\n",
    "triples = graph_gen.generate() # EMPTY TODO\n",
    "stop_time = time.process_time()\n",
    "print(stop_time)"
   ],
   "id": "7cf977b866e016ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "stop_time-start_time",
   "id": "5f59f96f58b87a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "graph_gen.free_model()",
   "id": "5d19179f036cd1a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# del graph_gen\n",
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "94511a4dea86b76f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "graph_gen_integration.draw_graph(triples)",
   "id": "e0353fa0d4bfb5e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"H\")",
   "id": "213b71600a018cf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(triples)",
   "id": "c6d6dd0a7fa722f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Check accuracy\n",
    "### Network"
   ],
   "id": "9eccbfd2e391ab08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "nblog = open(\"acc.log\", \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 5"
   ],
   "id": "eb95e59c5879b052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load testing dataset\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "graph_gen_dataset = pd.read_csv('data/webnlg_test.csv', header=0, delimiter='\\t')"
   ],
   "id": "e9cb5d0faa277155",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def select_k(spectrum, minimum_energy = 0.9):\n",
    "    running_total = 0.0\n",
    "    total = sum(spectrum)\n",
    "    if total == 0.0:\n",
    "        return len(spectrum)\n",
    "    for i in range(len(spectrum)):\n",
    "        running_total += spectrum[i]\n",
    "        if running_total / total >= minimum_energy:\n",
    "            return i + 1\n",
    "    return len(spectrum)"
   ],
   "id": "1db6dd0216d91b14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from graph_gen_integration import Triple\n",
    "import graph_gen_integration\n",
    "graph_gen = graph_gen_integration.GraphGen([], keyword_reward=1.1)"
   ],
   "id": "1f1362d80cfff837",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "similarities = []\n",
    "for idx, data in graph_gen_dataset.iterrows():\n",
    "    if idx >= 300:\n",
    "        break\n",
    "\n",
    "    print(f\"No. {idx + 1}\")\n",
    "    \n",
    "    # ground truth\n",
    "    graph1 = nx.DiGraph()\n",
    "\n",
    "    _triples = []\n",
    "\n",
    "    for _triple in data[\"Triples\"].split(\"<T>\"):\n",
    "        print(_triple)\n",
    "        try:\n",
    "            if _triple == \"\":\n",
    "                continue\n",
    "            split = re.split(\"<R>|<S>\", _triple)\n",
    "            subject = split[0]\n",
    "            predicate = split[1]\n",
    "            _object = split[2]\n",
    "\n",
    "            graph1.add_edge(subject, _object)\n",
    "            _triples.append(Triple(subject, predicate, _object, 0, 0))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"NON-STANDARD TRIPLE {_triple} ({e})\")\n",
    "            continue\n",
    "\n",
    "\n",
    "    print(_triples)\n",
    "\n",
    "    # Predicted\n",
    "    graph_gen.reset_model([_triples[0]], 0)\n",
    "    triples = graph_gen.generate(data[\"NL\"])\n",
    "\n",
    "    triples.append(_triples[0])\n",
    "\n",
    "    graph2 = nx.DiGraph()\n",
    "    \n",
    "    for triple in triples:\n",
    "        graph2.add_edge(triple.subject, triple.object)\n",
    "    \n",
    "    # Similarity\n",
    "    laplacian1 = nx.spectrum.laplacian_spectrum(graph1)\n",
    "    laplacian2 = nx.spectrum.laplacian_spectrum(graph2)\n",
    "\n",
    "    k1 = select_k(laplacian1)\n",
    "    k2 = select_k(laplacian2)\n",
    "    k = min(k1, k2)\n",
    "\n",
    "    similarity = sum((laplacian1[:k] - laplacian2[:k])**2)\n",
    "    print(similarity)\n",
    "    similarities.append(similarity)\n"
   ],
   "id": "119fac8936f8cdfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "print(similarities)\n",
    "print(f\"avg: {np.average(similarities)}\")"
   ],
   "id": "e65ccc88fdb237fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gemini (online and really slow arghhh)",
   "id": "93a6faebb3eaedcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Extract as much information from the Input as possible, and express the relationships in RDF Triples that compliment the existing RDF triples created from previous text. Do not use information from common sense. Expressed information must be confidently present in the text.\n",
    "### Example:\n",
    "(context)\n",
    "<T>Turkey<R>largestCity<S>Istanbul\n",
    "(input)\n",
    "Turkish Nurhan Atasoy was born and raised in the country of Turkey, home of the city of Istanbul and the lira currency.\n",
    "(output)\n",
    "<T>Nurhan_Atasoy<R>nationality<S>Turkish_people<T>Turkey<R>currency<S>Turkish_lira<T>Nurhan_Atasoy<R>birthPlace<S>Turkey\n",
    "### Existing RDF triples:\n",
    "{}\n",
    "### Input:\n",
    "{}\"\"\"\n",
    "import google.generativeai as gemini\n",
    "import os\n",
    "\n",
    "gemini.configure(api_key=\"\") # TODO: GEMINI KEY\n",
    "\n",
    "similarities = []\n",
    "_max = 100\n",
    "for idx, data in graph_gen_dataset.iterrows():\n",
    "    print(f\"No. {idx + 1}/{_max}\")\n",
    "    if idx >= 50:\n",
    "        break\n",
    "    \n",
    "    # ground truth\n",
    "    graph1 = nx.DiGraph()\n",
    "\n",
    "    _triples = []\n",
    "\n",
    "    for _triple in data[\"Triples\"].split(\"<T>\"):\n",
    "        print(_triple)\n",
    "        try:\n",
    "            if _triple == \"\":\n",
    "                continue\n",
    "            split = re.split(\"<R>|<S>\", _triple)\n",
    "            subject = split[0]\n",
    "            predicate = split[1]\n",
    "            _object = split[2]\n",
    "\n",
    "            graph1.add_edge(subject, _object)\n",
    "            _triples.append(Triple(subject, predicate, _object, 0, 0))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"NON-STANDARD TRIPLE {_triple} ({e})\")\n",
    "            continue\n",
    "            \n",
    "    # Gemini\n",
    "    model = gemini.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(prompt.format(_triples[0], data[\"NL\"], \"\"))\n",
    "    print(response)\n",
    "\n",
    "    # print(prompt.format(data[\"NL\"], \"\").removesuffix(\"<unused0>\"))\n",
    "    # print(completion.choices[0].message.content)\n",
    "    graph2 = nx.DiGraph()\n",
    "\n",
    "    graph2.add_edge(_triples[0].subject, _triples[0].object)\n",
    "\n",
    "    for _triple in response.text.split(\"<T>\"):\n",
    "        print(_triple)\n",
    "        try:\n",
    "            if _triple == \"\":\n",
    "                continue\n",
    "            split = re.split(\"<R>|<S>\", _triple)\n",
    "            subject = split[0]\n",
    "            predicate = split[1]\n",
    "            _object = split[2]\n",
    "\n",
    "            graph2.add_edge(subject, _object)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"NON-STANDARD TRIPLE {_triple} ({e})\")\n",
    "            continue\n",
    "            \n",
    "    # Similarity\n",
    "    laplacian1 = nx.spectrum.laplacian_spectrum(graph1)\n",
    "    laplacian2 = nx.spectrum.laplacian_spectrum(graph2)\n",
    "\n",
    "    k1 = select_k(laplacian1)\n",
    "    k2 = select_k(laplacian2)\n",
    "    k = min(k1, k2)\n",
    "\n",
    "    similarity = sum((laplacian1[:k] - laplacian2[:k])**2)\n",
    "    print(similarity)\n",
    "    similarities.append(similarity)\n",
    "\n"
   ],
   "id": "a23e2dae7cdfb8a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "similarities",
   "id": "2a00b4a82fb5f3aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "np.median(similarities)"
   ],
   "id": "58ac7908d3191271",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gemma (local thingy)",
   "id": "4d2e02d38261638f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Gemma 2b it\n",
    "import os\n",
    "\n",
    "from GraphGen.graph_gen_integration import Triple\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "class Gemma2bItGen:\n",
    "    prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Extract as much information from the Input as possible, and express the relationships in RDF Triples that compliment the existing RDF triples created from previous text. Do not use information from common sense. Expressed information must be confidently present in the text.\n",
    "### Example:\n",
    "(context)\n",
    "<T>Turkey<R>largestCity<S>Istanbul\n",
    "(input)\n",
    "Turkish Nurhan Atasoy was born and raised in the country of Turkey, home of the city of Istanbul and the lira currency.\n",
    "(output)\n",
    "<T>Nurhan_Atasoy<R>nationality<S>Turkish_people<T>Turkey<R>currency<S>Turkish_lira<T>Nurhan_Atasoy<R>birthPlace<S>Turkey\n",
    "### Existing RDF triples:\n",
    "{}\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # init tokenizer, models, etc.\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "        # self.max_chunk_length = max_chunk_size\n",
    "        # self.keyword_reward = keyword_reward\n",
    "\n",
    "        self.setup_model()\n",
    "\n",
    "    def setup_model(self):\n",
    "        # setup model helper function\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"../gemma-2-9b-it\",\n",
    "            max_seq_length=2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        FastLanguageModel.for_inference(self.model)  # Enable native 2x faster inference\n",
    "\n",
    "    def free_model(self):\n",
    "        # Free model from GPU memory to load other models\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def reset_model(self, existing_rdfs):\n",
    "        pass\n",
    "    \n",
    "    def generate(self, _triples, input_text):\n",
    "        inputs = self.tokenizer(\n",
    "            [\n",
    "                self.prompt.format(\n",
    "                    input_text,\n",
    "                    _triples,  # output - leave this blank for generation!\n",
    "                )\n",
    "            ], return_tensors=\"pt\").to(\"cuda\")\n",
    "        print(self.prompt.format(\n",
    "                    input_text,\n",
    "                    \"\",  # output - leave this blank for generation!\n",
    "                ))\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "\n",
    "            # temperature = 0.9,\n",
    "            # max_new_tokens = max(len(text) + 100, 700),\n",
    "            # num_beams = 3,\n",
    "            # early_stopping = True,\n",
    "\n",
    "            use_cache=True,\n",
    "            max_new_tokens = 500\n",
    "            # Use cache = false is broken haha, but beam search is broken when not using cache hahahah ;-;\n",
    "        )\n",
    "        response = self.tokenizer.batch_decode(outputs)\n",
    "        print(response)\n",
    "        answer_text = response[0].split(\"### Response:\\n\")[1]\n",
    "        graph2 = nx.DiGraph()\n",
    "        for _triple in answer_text.split(\"<T>\"):\n",
    "            print(_triple)\n",
    "            try:\n",
    "                if _triple == \"\":\n",
    "                    continue\n",
    "                split = re.split(\"<R>|<S>\", _triple)\n",
    "                subject = split[0]\n",
    "                predicate = split[1]\n",
    "                _object = split[2]\n",
    "    \n",
    "                graph2.add_edge(subject, _object)\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(f\"NON-STANDARD TRIPLE {_triple} ({e})\")\n",
    "                continue\n",
    "        \n",
    "        return graph2"
   ],
   "id": "b2d6ba8356de2b49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "it_gen = Gemma2bItGen()",
   "id": "1dddda9267127330",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# del it_gen\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "4f0868aba063142b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "graph_gen_dataset = pd.read_csv('data/webnlg_test.csv', header=0, delimiter='\\t')\n",
    "\n",
    "\n",
    "def select_k(spectrum, minimum_energy = 0.9):\n",
    "    running_total = 0.0\n",
    "    total = sum(spectrum)\n",
    "    if total == 0.0:\n",
    "        return len(spectrum)\n",
    "    for i in range(len(spectrum)):\n",
    "        running_total += spectrum[i]\n",
    "        if running_total / total >= minimum_energy:\n",
    "            return i + 1\n",
    "    return len(spectrum)"
   ],
   "id": "c021ab5746c93789",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "similarities = []\n",
    "for idx, data in graph_gen_dataset.iterrows():\n",
    "    print(f\"No. {idx+1}\")\n",
    "    if idx >= 500:\n",
    "        break\n",
    "    \n",
    "    # ground truth\n",
    "    graph1 = nx.DiGraph()\n",
    "\n",
    "    _t = data[\"Triples\"].split(\"<T>\")[0]\n",
    "    if _t == \"\":\n",
    "        continue\n",
    "    split = re.split(\"<R>|<S>\", _t)\n",
    "    subject = split[0]\n",
    "    predicate = split[1]\n",
    "    _object = split[2]\n",
    "\n",
    "    _t_org = f\"<T>{subject}<R>{predicate}<S>{_object}\"\n",
    "\n",
    "\n",
    "\n",
    "    for _triple in data[\"Triples\"].split(\"<T>\"):\n",
    "        print(_triple)\n",
    "        try:\n",
    "            if _triple == \"\":\n",
    "                continue\n",
    "            split = re.split(\"<R>|<S>\", _triple)\n",
    "            subject = split[0]\n",
    "            predicate = split[1]\n",
    "            _object = split[2]\n",
    "\n",
    "            graph1.add_edge(subject, _object)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"NON-STANDARD TRIPLE {_triple} ({e})\")\n",
    "            continue\n",
    "            \n",
    "    # OpenAI\n",
    "    \n",
    "    # print(prompt.format(data[\"NL\"], \"\").removesuffix(\"<unused0>\"))\n",
    "    graph2 = it_gen.generate(_t_org, data[\"NL\"])\n",
    "            \n",
    "    # Similarity\n",
    "    laplacian1 = nx.spectrum.laplacian_spectrum(graph1)\n",
    "    laplacian2 = nx.spectrum.laplacian_spectrum(graph2)\n",
    "\n",
    "    k1 = select_k(laplacian1)\n",
    "    k2 = select_k(laplacian2)\n",
    "    k = min(k1, k2)\n",
    "\n",
    "    similarity = sum((laplacian1[:k] - laplacian2[:k])**2)\n",
    "    print(similarity)\n",
    "    similarities.append(similarity)"
   ],
   "id": "9f3099916f4f791f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(similarities)",
   "id": "424480005e8ca0ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"MED\")\n",
    "print(np.median(similarities))\n",
    "print(\"MEAN\")\n",
    "print(np.mean(similarities))"
   ],
   "id": "fe12839ba45bf810",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
