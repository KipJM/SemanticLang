{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# showcases original graph gen\n",
    "import math\n",
    "input_text = \"\"\"\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_chunck_length = 400\n",
    "import nltk.data\n",
    "\n",
    "pkt_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = pkt_tokenizer.tokenize(input_text)\n",
    "\n",
    "merged_sentences = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    if i >= 1 and len(merged_sentences[-1]) + len(sentence) <= max_chunck_length:\n",
    "        merged_sentences[-1] += \" \" + sentence\n",
    "    else:\n",
    "        if i >= 1:\n",
    "            merged_sentences.append(sentences[i - 1])\n",
    "        merged_sentences.append(sentence)\n",
    "\n",
    "merged_sentences"
   ],
   "id": "b2c89209ed07b795",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"graph_2b_test\",\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "    \n",
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Extract the most confident information in the sentence below as much as possible, and express the relationships in RDF Triples that complement the existing RDF triples. Do not use information from common sense.\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "<T>{}\"\"\""
   ],
   "id": "261181259f31faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for chunk in merged_sentences:\n",
    "    print(f'####proc || {chunk}')\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            prompt.format(\n",
    "                chunk,  # input\n",
    "                \"\",  # output - leave this blank for generation!\n",
    "            )\n",
    "        ], return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        \n",
    "        # temperature = 0.9,\n",
    "        # max_new_tokens = max(len(text) + 100, 700), \n",
    "        # num_beams = 3,\n",
    "        # early_stopping = True,\n",
    "        \n",
    "        use_cache=True, # Use cache = false is broken haha, but beam search is broken when not using cache hahahah ;-;\n",
    "    )\n",
    "    response = tokenizer.batch_decode(outputs)\n",
    "    print(response)"
   ],
   "id": "17bbfb7e774fb683",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8b25cbafa11bf89b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
