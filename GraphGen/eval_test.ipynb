{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "    \"unsloth/gemma-2-2b-bnb-4bit\",             # New small Gemma model!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-2-2b-it-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Extract the most confident information in the sentence below as much as possible, and express the relationships in RDF Triples that complement the existing RDF triples. Do not use information from common sense.\n",
    "### Example:\n",
    "Input:Marriott International, with Bill Marriott as a key leader, is the tenant of AC Hotel Bella Sky in Copenhagen.\n",
    "Response:<T>AC_Hotel_Bella_Sky_Copenhagen<R>location<S>Copenhagen<T>AC_Hotel_Bella_Sky_Copenhagen<R>tenant<S>Marriott_International<T>Marriott_International<R>keyPerson<S>Bill_Marriott\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "{}\"\"\""
   ],
   "id": "6363297cb0a81f59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        \"\"\"Turn Me On is a 35.1 minute long album produced by Wharton Tiers that was followed by the album entitled Take it Off.\"\"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 500)"
   ],
   "id": "7dd8466df0da094c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "graph_gen_dataset = pd.read_csv('data/webnlg_test.csv', header=0, delimiter='\\t')\n",
    "\n",
    "\n",
    "def select_k(spectrum, minimum_energy = 0.9):\n",
    "    running_total = 0.0\n",
    "    total = sum(spectrum)\n",
    "    if total == 0.0:\n",
    "        return len(spectrum)\n",
    "    for i in range(len(spectrum)):\n",
    "        running_total += spectrum[i]\n",
    "        if running_total / total >= minimum_energy:\n",
    "            return i + 1\n",
    "    return len(spectrum)"
   ],
   "id": "8b69b665693f899d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "similarities = []\n",
    "for idx, data in graph_gen_dataset.iterrows():\n",
    "    print(idx)\n",
    "    if idx >= 50:\n",
    "        break\n",
    "    \n",
    "    # ground truth\n",
    "    graph1 = nx.DiGraph()\n",
    "\n",
    "    for _triple in data[\"Triples\"].split(\"<T>\"):\n",
    "        print(_triple)\n",
    "        try:\n",
    "            if _triple == \"\":\n",
    "                continue\n",
    "            split = re.split(\"<R>|<S>\", _triple)\n",
    "            subject = split[0]\n",
    "            predicate = split[1]\n",
    "            _object = split[2]\n",
    "\n",
    "            graph1.add_edge(subject, _object)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"NON-STANDARD TRIPLE {_triple} ({e})\")\n",
    "            continue\n",
    "            \n",
    "    # OpenAI\n",
    "    \n",
    "    # print(prompt.format(data[\"NL\"], \"\").removesuffix(\"<unused0>\"))\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        prompt.format(\n",
    "            \"\"\"Turn Me On is a 35.1 minute long album produced by Wharton Tiers that was followed by the album entitled Take it Off.\"\"\", # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "    from transformers import TextStreamer\n",
    "    out = model.generate(**inputs, max_new_tokens = 500)\n",
    "    response = tokenizer.batch_decode(out)\n",
    "    \n",
    "    answer_text = response[0].split(\"### Response:\\n\")[1]\n",
    "    graph2 = nx.DiGraph()\n",
    "    for _triple in answer_text.split(\"<T>\"):\n",
    "        print(_triple)\n",
    "        try:\n",
    "            if _triple == \"\":\n",
    "                continue\n",
    "            split = re.split(\"<R>|<S>\", _triple)\n",
    "            subject = split[0]\n",
    "            predicate = split[1]\n",
    "            _object = split[2]\n",
    "\n",
    "            graph2.add_edge(subject, _object)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"NON-STANDARD TRIPLE {_triple} ({e})\")\n",
    "            continue\n",
    "            \n",
    "    # Similarity\n",
    "    laplacian1 = nx.spectrum.laplacian_spectrum(graph1)\n",
    "    laplacian2 = nx.spectrum.laplacian_spectrum(graph2)\n",
    "\n",
    "    k1 = select_k(laplacian1)\n",
    "    k2 = select_k(laplacian2)\n",
    "    k = min(k1, k2)\n",
    "\n",
    "    similarity = sum((laplacian1[:k] - laplacian2[:k])**2)\n",
    "    print(similarity)\n",
    "    similarities.append(similarity)"
   ],
   "id": "3893e133b941f653",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import math\n",
    "similarities"
   ],
   "id": "e74186adf28011b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "np.average(similarities)"
   ],
   "id": "cecf18530c1e5cfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "967884dfdd5233cc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
