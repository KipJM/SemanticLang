{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Unsloth",
   "id": "92314dc607fbef32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:13:14.155376Z",
     "start_time": "2024-11-12T04:13:14.143481Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Jupyter running!\")",
   "id": "57bdfc3f2b145521",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter running!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Logging",
   "id": "21941613da619b16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T03:58:20.210026Z",
     "start_time": "2024-11-12T03:58:20.188423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "nblog = open(\"nb.log\", \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 5"
   ],
   "id": "a900d1cd894cd07c",
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(5000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T03:58:23.787489Z",
     "start_time": "2024-11-12T03:58:20.830192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            print(logs)"
   ],
   "id": "633b1bfad78e1eee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:00.186913Z",
     "start_time": "2024-11-12T03:58:23.857229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "    \"unsloth/gemma-2-2b-bnb-4bit\",             # New small Gemma model!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../gemma-2-9b\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ],
   "id": "f42fbdfc1c34ed03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.11.5: Fast Gemma2 patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 30.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:05.548603Z",
     "start_time": "2024-11-12T04:00:00.201397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ],
   "id": "1d2310569f1a7f85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.5 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load datasets",
   "id": "e128df57c0d34ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:05.764429Z",
     "start_time": "2024-11-12T04:00:05.614818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "df = pd.read_csv(\"data/webnlg_train.csv\", header=0, delimiter='\\t')"
   ],
   "id": "a4785be70c6c36f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:08.257107Z",
     "start_time": "2024-11-12T04:00:05.800065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "texts = []\n",
    "for index, row in df.iterrows():\n",
    "    output = row['Triples']\n",
    "    p_input = row[\"NL\"]\n",
    "    triples = output.split('<T>')\n",
    "    triples = [\"<T>\" + triple for triple in filter(None, triples)]\n",
    "\n",
    "    if len(triples) > 1:\n",
    "        # 1 context\n",
    "        if len(triples) >= 2:\n",
    "            context = \"\".join(triples[0])\n",
    "            sliced_output = \"\".join(triples[1:])\n",
    "            texts.append([context, p_input, sliced_output])\n",
    "\n",
    "        # 2 context\n",
    "        if len(triples) >= 3:\n",
    "            context = \"\".join(triples[0:2])\n",
    "            sliced_output = \"\".join(triples[2:])\n",
    "            texts.append([context, p_input, sliced_output])\n",
    "\n",
    "        # 3 context\n",
    "        if len(triples) >= 4:\n",
    "            context = \"\".join(triples[0:3])\n",
    "            sliced_output = \"\".join(triples[3:])\n",
    "            texts.append([context, p_input, sliced_output])\n",
    "\n",
    "    # Original\n",
    "    texts.append([\"None\", p_input, output])\n",
    "\n",
    "# df\n",
    "df = pd.DataFrame(texts, columns = [\"context\", \"input\", \"output\"])\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df"
   ],
   "id": "7af070e7f81d93ab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 context  \\\n",
       "0      <T>United_States<R>ethnicGroup<S>Native_Americ...   \n",
       "1              <T>Agnes_Kant<R>nationality<S>Netherlands   \n",
       "2      <T>Alan_Martin_(footballer)<R>club<S>Hamilton_...   \n",
       "3                                                   None   \n",
       "4      <T>14th_New_Jersey_Volunteer_Infantry_Monument...   \n",
       "...                                                  ...   \n",
       "97131                                               None   \n",
       "97132  <T>103_Hera<R>discoverer<S>James_Craig_Watson<...   \n",
       "97133  <T>Andrew_Rayel<R>associatedBand/associatedMus...   \n",
       "97134                                               None   \n",
       "97135                                               None   \n",
       "\n",
       "                                                   input  \\\n",
       "0      The United States includes the Native American...   \n",
       "1      The leader of the Netherlands is Mark Rutte wh...   \n",
       "2      The footballer Alan Martin has played for Hami...   \n",
       "3      Abilene Regional Airport serves the city of Ab...   \n",
       "4      The 14th New Jersey Volunteer Infantry Monumen...   \n",
       "...                                                  ...   \n",
       "97131  The birth place of Allan Shivers is Lufkin, Te...   \n",
       "97132  James Craig Watson, who discovered 103 Hera, o...   \n",
       "97133  Trance musician Andrew Rayel is associated wit...   \n",
       "97134         Ahmet Ertegun's genre is rhythm and blues.   \n",
       "97135  Bandeja paisa, which uses chorizo, is typical ...   \n",
       "\n",
       "                                                  output  \n",
       "0      <T>Auburn,_Alabama<R>country<S>United_States<T...  \n",
       "1      <T>Netherlands<R>leader<S>Mark_Rutte<T>Agnes_K...  \n",
       "2      <T>Alan_Martin_(footballer)<R>club<S>Aldershot...  \n",
       "3      <T>Abilene_Regional_Airport<R>cityServed<S>Abi...  \n",
       "4      <T>14th_New_Jersey_Volunteer_Infantry_Monument...  \n",
       "...                                                  ...  \n",
       "97131      <T>Allan_Shivers<R>birthPlace<S>Lufkin,_Texas  \n",
       "97132   <T>James_Craig_Watson<R>deathCause<S>Peritonitis  \n",
       "97133  <T>Andrew_Rayel<R>associatedBand/associatedMus...  \n",
       "97134        <T>Ahmet_Ertegun<R>genre<S>Rhythm_and_blues  \n",
       "97135  <T>Bandeja_paisa<R>ingredient<S>Chorizo<T>Band...  \n",
       "\n",
       "[97136 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;T&gt;United_States&lt;R&gt;ethnicGroup&lt;S&gt;Native_Americ...</td>\n",
       "      <td>The United States includes the Native American...</td>\n",
       "      <td>&lt;T&gt;Auburn,_Alabama&lt;R&gt;country&lt;S&gt;United_States&lt;T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;T&gt;Agnes_Kant&lt;R&gt;nationality&lt;S&gt;Netherlands</td>\n",
       "      <td>The leader of the Netherlands is Mark Rutte wh...</td>\n",
       "      <td>&lt;T&gt;Netherlands&lt;R&gt;leader&lt;S&gt;Mark_Rutte&lt;T&gt;Agnes_K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;T&gt;Alan_Martin_(footballer)&lt;R&gt;club&lt;S&gt;Hamilton_...</td>\n",
       "      <td>The footballer Alan Martin has played for Hami...</td>\n",
       "      <td>&lt;T&gt;Alan_Martin_(footballer)&lt;R&gt;club&lt;S&gt;Aldershot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>Abilene Regional Airport serves the city of Ab...</td>\n",
       "      <td>&lt;T&gt;Abilene_Regional_Airport&lt;R&gt;cityServed&lt;S&gt;Abi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;T&gt;14th_New_Jersey_Volunteer_Infantry_Monument...</td>\n",
       "      <td>The 14th New Jersey Volunteer Infantry Monumen...</td>\n",
       "      <td>&lt;T&gt;14th_New_Jersey_Volunteer_Infantry_Monument...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97131</th>\n",
       "      <td>None</td>\n",
       "      <td>The birth place of Allan Shivers is Lufkin, Te...</td>\n",
       "      <td>&lt;T&gt;Allan_Shivers&lt;R&gt;birthPlace&lt;S&gt;Lufkin,_Texas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97132</th>\n",
       "      <td>&lt;T&gt;103_Hera&lt;R&gt;discoverer&lt;S&gt;James_Craig_Watson&lt;...</td>\n",
       "      <td>James Craig Watson, who discovered 103 Hera, o...</td>\n",
       "      <td>&lt;T&gt;James_Craig_Watson&lt;R&gt;deathCause&lt;S&gt;Peritonitis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97133</th>\n",
       "      <td>&lt;T&gt;Andrew_Rayel&lt;R&gt;associatedBand/associatedMus...</td>\n",
       "      <td>Trance musician Andrew Rayel is associated wit...</td>\n",
       "      <td>&lt;T&gt;Andrew_Rayel&lt;R&gt;associatedBand/associatedMus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97134</th>\n",
       "      <td>None</td>\n",
       "      <td>Ahmet Ertegun's genre is rhythm and blues.</td>\n",
       "      <td>&lt;T&gt;Ahmet_Ertegun&lt;R&gt;genre&lt;S&gt;Rhythm_and_blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97135</th>\n",
       "      <td>None</td>\n",
       "      <td>Bandeja paisa, which uses chorizo, is typical ...</td>\n",
       "      <td>&lt;T&gt;Bandeja_paisa&lt;R&gt;ingredient&lt;S&gt;Chorizo&lt;T&gt;Band...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97136 rows Ã— 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:08.538748Z",
     "start_time": "2024-11-12T04:00:08.345887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ],
   "id": "9d004f4e39378d59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'input', 'output'],\n",
       "    num_rows: 97136\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:08.599608Z",
     "start_time": "2024-11-12T04:00:08.591130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Extract the most confident information in the sentence below as much as possible, and express the relationships in RDF Triples that complement the existing RDF triples. Do not use information from common sense.\n",
    "### Existing RDF triples:\n",
    "{}\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "<unused0>{}\"\"\"\n",
    "EOS_TOKEN = tokenizer.eos_token # must add!\n",
    "\n",
    "# Format dataset\n",
    "def format_prompts(examples):\n",
    "    context = examples[\"context\"]\n",
    "    inputs  = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for _context, _input, _output in zip(context, inputs, outputs):\n",
    "        _context = _context.replace(\"<T>\", \"<unused0>\") # change to single token\n",
    "        _context = _context.replace(\"<R>\", \"<unused1>\") # change to single token\n",
    "        _context = _context.replace(\"<S>\", \"<unused2>\") # change to single token\n",
    "        \n",
    "        _output = _output[3:] # remove the first <T> because the prompt contains that\n",
    "        _output = _output.replace(\"<T>\", \"<unused0>\") # change to single token\n",
    "        _output = _output.replace(\"<R>\", \"<unused1>\") # change to single token\n",
    "        _output = _output.replace(\"<S>\", \"<unused2>\") # change to single token\n",
    "        \n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = prompt.format(_context, _input, _output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "\n",
    "# response_template = \"### Response:\"\n",
    "# collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ],
   "id": "617be11b6803dcd8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:09.981552Z",
     "start_time": "2024-11-12T04:00:08.668216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "dataset"
   ],
   "id": "ca7686070882ca35",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/97136 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fda7e58e2b341d39b593a93500e2698"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'input', 'output', 'text'],\n",
       "    num_rows: 97136\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:33.147928Z",
     "start_time": "2024-11-12T04:00:10.018012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    dataset_text_field=\"text\",\n",
    "    # data_collator=collator, # Response only gen\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 1000,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ],
   "id": "19962502e3d305c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/97136 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1325b96542004bdb952822357fa3a10a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:33.405151Z",
     "start_time": "2024-11-12T04:00:33.396556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ],
   "id": "3485cd974aeb42d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100 80GB PCIe. Max memory = 30.0 GB.\n",
      "6.576 GB of memory reserved.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T04:00:33.434928Z",
     "start_time": "2024-11-12T04:00:33.428614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "log_callback = PrinterCallback()\n",
    "trainer.add_callback(log_callback)"
   ],
   "id": "6952dc9dad90a8c0",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-12T04:00:33.560238Z"
    }
   },
   "cell_type": "code",
   "source": "trainer_stats = trainer.train()",
   "id": "e3c6b9efd3a31324",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T03:57:42.482018700Z",
     "start_time": "2024-09-14T04:46:19.641049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ],
   "id": "68b44c2aa2d59cc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3528.5272 seconds used for training.\n",
      "58.81 minutes used for training.\n",
      "Peak reserved memory = 4.053 GB.\n",
      "Peak reserved memory for training = 0.088 GB.\n",
      "Peak reserved memory % of max memory = 67.55 %.\n",
      "Peak reserved memory for training % of max memory = 1.467 %.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T03:57:42.482018700Z",
     "start_time": "2024-09-14T04:46:19.953518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # alpaca_prompt = Copied from above\n",
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "# inputs = tokenizer(\n",
    "# [\n",
    "#     prompt.format(\n",
    "#         \"\", # input\n",
    "#         \"\", # output - leave this blank for generation!\n",
    "#     )\n",
    "# ], return_tensors = \"pt\").to(\"cuda\")\n",
    "# \n",
    "# outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "# tokenizer.batch_decode(outputs)"
   ],
   "id": "1c8057f37e67ab1c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T03:57:42.515220800Z",
     "start_time": "2024-09-14T04:46:20.022817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt.format(\n",
    "        \"<unused0>Turn_Me_On_(album)<unused1>runtime<unused1>35.1\",\n",
    "        \"\"\"Turn Me On is a 35.1 minute long album produced by Wharton Tiers that was followed by the album entitled Take it Off.\"\"\", # input\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 500)"
   ],
   "id": "d9ad4b0d0a06e624",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "Extract the most confident information in the sentence below as much as possible, and express the relationships in RDF Triples that complement the existing RDF triples. Do not use information from common sense.\n",
      "### Existing RDF triples:\n",
      "<unused0>Turn_Me_On_(album)<unused1>runtime<unused1>35.1\n",
      "### Input:\n",
      "Turn Me On is a 35.1 minute long album produced by Wharton Tiers that was followed by the album entitled Take it Off.\n",
      "### Response:\n",
      "<unused0>Turn_Me_On_(album)<unused1>followedBy<unused2>Take_It_Off_(album)<unused0>Take_It_Off_(album)<unused1>producer<unused2>Wharton_Tier<eos>\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Saving",
   "id": "361edc3de33e1e33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T03:57:42.515220800Z",
     "start_time": "2024-09-14T04:46:36.329945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"graph_9b_full\") # Local saving\n",
    "tokenizer.save_pretrained(\"graph_9b_full\")"
   ],
   "id": "ead2607d0b91440b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('graph_2b_1000step/tokenizer_config.json',\n",
       " 'graph_2b_1000step/special_tokens_map.json',\n",
       " 'graph_2b_1000step/tokenizer.model',\n",
       " 'graph_2b_1000step/added_tokens.json',\n",
       " 'graph_2b_1000step/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference",
   "id": "a91a9368cc6a8e94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T03:57:42.515220800Z",
     "start_time": "2024-09-09T05:35:54.606543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if True:\n",
    "#     from unsloth import FastLanguageModel\n",
    "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#         model_name = \"graph_2b_test\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "#         max_seq_length = 2048,\n",
    "#         dtype = None,\n",
    "#         load_in_4bit = True,\n",
    "#     )\n",
    "#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "# \n",
    "# # alpaca_prompt = You MUST copy from above!\n",
    "# \n",
    "# prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "# ### Instruction:\n",
    "# Using only the information provided in the sentence, extract the relationship between subjects in the sentence below, and express the relationships in RDF Triples.\n",
    "# ### Input:\n",
    "# {}\n",
    "# ### Response:\n",
    "# {}\"\"\"\n",
    "# \n",
    "# inputs = tokenizer(\n",
    "# [\n",
    "#     prompt.format(\n",
    "#         \"The AP CSA course is a college-level course developed by Collegeboard, it teaches high-school students the basic of programming in Java.\", # input\n",
    "#         \"\", # output - leave this blank for generation!\n",
    "#     )\n",
    "# ], return_tensors = \"pt\").to(\"cuda\")\n",
    "# \n",
    "# from transformers import TextStreamer\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 100)"
   ],
   "id": "797d51ec93077733",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2060. Max memory: 6.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01munsloth\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FastLanguageModel\n\u001B[0;32m----> 3\u001B[0m     model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mFastLanguageModel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgemma2_2b_test\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# YOUR MODEL YOU USED FOR TRAINING\u001B[39;49;00m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     FastLanguageModel\u001B[38;5;241m.\u001B[39mfor_inference(model) \u001B[38;5;66;03m# Enable native 2x faster inference\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# alpaca_prompt = You MUST copy from above!\u001B[39;00m\n",
      "File \u001B[0;32m~/.virtualenvs/SemanticLang/lib/python3.10/site-packages/unsloth/models/loader.py:301\u001B[0m, in \u001B[0;36mFastLanguageModel.from_pretrained\u001B[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001B[0m\n\u001B[1;32m    298\u001B[0m     tokenizer_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m--> 301\u001B[0m model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mdispatch_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    302\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m        \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    304\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m             \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[43m    \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[43m      \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    306\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m             \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m        \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    308\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrope_scaling\u001B[49m\u001B[43m      \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mrope_scaling\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfix_tokenizer\u001B[49m\u001B[43m     \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mfix_tokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_patcher\u001B[49m\u001B[43m     \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdispatch_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer_name\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtokenizer_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m          \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mrevision\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_peft\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    315\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resize_model_vocab \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    318\u001B[0m     model\u001B[38;5;241m.\u001B[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001B[0;32m~/.virtualenvs/SemanticLang/lib/python3.10/site-packages/unsloth/models/llama.py:1580\u001B[0m, in \u001B[0;36mFastLlamaModel.from_pretrained\u001B[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001B[0m\n\u001B[1;32m   1577\u001B[0m \u001B[38;5;66;03m# Cannot be None, since HF now checks for the config\u001B[39;00m\n\u001B[1;32m   1578\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m load_in_4bit: kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_config\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m bnb_config\n\u001B[0;32m-> 1580\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1581\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1582\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m              \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1583\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m             \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1584\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# quantization_config     = bnb_config,\u001B[39;49;00m\n\u001B[1;32m   1585\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m                   \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1586\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_position_embeddings\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmax_position_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1587\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m       \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1588\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattn_implementation\u001B[49m\u001B[43m     \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meager\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1589\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1590\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1591\u001B[0m \u001B[38;5;66;03m# Return old flag\u001B[39;00m\n\u001B[1;32m   1592\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHF_HUB_ENABLE_HF_TRANSFER\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m old_hf_transfer\n",
      "File \u001B[0;32m~/.virtualenvs/SemanticLang/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[0;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    570\u001B[0m )\n",
      "File \u001B[0;32m~/.virtualenvs/SemanticLang/lib/python3.10/site-packages/transformers/modeling_utils.py:3909\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3906\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m infer_auto_device_map(model, dtype\u001B[38;5;241m=\u001B[39mtarget_dtype, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdevice_map_kwargs)\n\u001B[1;32m   3908\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 3909\u001B[0m         \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3911\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3912\u001B[0m     model\u001B[38;5;241m.\u001B[39mtie_weights()\n",
      "File \u001B[0;32m~/.virtualenvs/SemanticLang/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:86\u001B[0m, in \u001B[0;36mBnb4BitHfQuantizer.validate_environment\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     82\u001B[0m     device_map_without_lm_head \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     83\u001B[0m         key: device_map[key] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m device_map\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodules_to_not_convert\n\u001B[1;32m     84\u001B[0m     }\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisk\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[0;32m---> 86\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     87\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     88\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     89\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     90\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`from_pretrained`. Check \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     91\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     92\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor more details. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     93\u001B[0m         )\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(importlib\u001B[38;5;241m.\u001B[39mmetadata\u001B[38;5;241m.\u001B[39mversion(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbitsandbytes\u001B[39m\u001B[38;5;124m\"\u001B[39m)) \u001B[38;5;241m<\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.39.0\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     97\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     98\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     99\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19926b62a4325638"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
