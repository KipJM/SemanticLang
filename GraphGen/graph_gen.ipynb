{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c20d26bb908e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "input_text = \"\"\"Typhoon Yagi, known in the Philippines as Severe Tropical Storm Enteng, was a powerful and destructive tropical cyclone which impacted the Philippines, China and Vietnam in early September 2024. Yagi, which means goat or the constellation of Capricornus in Japanese, is the eleventh named storm, the first violent typhoon and Category 5 storm of the annual typhoon season. It was one of the most intense typhoons ever to strike northern Vietnam, the strongest typhoon to strike Hainan during the meteorological autumn and the strongest since Rammasun in 2014. It is one of only four Category 5 super typhoons recorded in the South China Sea, alongside Pamela in 1954, Rammasun in 2014 and Rai in 2021.\n",
    "\n",
    "Yagi originated from a low-pressure area that formed on August 30, approximately 540 km (330 mi) northwest of Palau. On September 1, the system was classified as a tropical storm and named Yagi by the Japan Meteorological Agency (JMA). After making landfall over Casiguran, Aurora in the Philippines, on September 2, Yagi weakened as it moved inland through the rugged terrain of the Cordillera Central of Luzon. It later emerged over the South China Sea and began merging with a secondary circulation west of Lingayen Gulf, with its deep convection starting to wrap and develop convective bands extending west and south. On September 5, the JMA reported that the storm reached its peak intensity with ten-minute sustained winds of 195 km/h (120 mph) and a central pressure of 915 hPa (27.02 inHg). It subsequently peaked as a Category 5-equivalent super typhoon on the Saffir-Simpson scale, with one-minute sustained winds of 260 km/h (160 mph). After weakening during an eyewall replacement cycle, Yagi slightly restrengthened before making landfall near Wenchang in China's Hainan Province on September 6. Yagi passed over northern Hainan and directly over Haikou, before briefly making landfall over Xuwen County in mainland Guangdong Province and moving into the open waters of the Gulf of Tonkin. It made landfall over Haiphong and Quang Ninh, Vietnam, on September 7 and moved southwestward inland until it was last noted on September 8.\n",
    "\n",
    "The combination of Yagi and the southwest monsoon led to heavy rains over Luzon, causing widespread flash floods in various areas. The Hong Kong Observatory issued a Gale or Storm No. 8 for Hong Kong as Typhoon Yagi approached. Power outages and downed trees were reported in Hainan; in preparation for Typhoon Yagi, schools in areas in the trajectory of the storm were closed along with local transport services across the island province. In Vietnam, several structures including electric poles were uprooted, leading to power outages in various areas. In total, the typhoon caused at least 48 deaths, 321 injuries, and left 39 people missing, resulting in US$9.29 billion in damage across several countries. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5897bfbad8661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9a465f4e531859",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_chunck_length = 400\n",
    "import nltk.data\n",
    "\n",
    "pkt_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "sentences = pkt_tokenizer.tokenize(input_text)\n",
    "\n",
    "merged_sentences = []\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    sentence = sentences[i]\n",
    "    if i >= 1 and len(merged_sentences[-1]) + len(sentence) <= max_chunck_length:\n",
    "        merged_sentences[-1] += \" \" + sentence\n",
    "    else:\n",
    "        if i >= 1:\n",
    "            merged_sentences.append(sentences[i - 1])\n",
    "        merged_sentences.append(sentence)\n",
    "\n",
    "merged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2959ce081f95e37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T06:18:41.842500Z",
     "start_time": "2024-11-12T06:18:11.765415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2060. Max memory: 6.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"./graph_2b_tokenfix\",\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57a0f394f1d764",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T06:19:51.575865Z",
     "start_time": "2024-11-12T06:19:51.569856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 2], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"<bos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cd3dbd698ac69",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Custom Logit Processors\n",
    "import random\n",
    "import torch\n",
    "from transformers.generation import LogitsProcessor, LogitsProcessorList\n",
    "\n",
    "def disable_tokens(scores, banned_tokens):\n",
    "    for token in banned_tokens:\n",
    "        scores[0][token] = -math.inf\n",
    "    return scores\n",
    "\n",
    "\n",
    "def find_largest_index(lst: list, value):\n",
    "    try:\n",
    "        return len(lst) - 1 - lst[::-1].index(value)\n",
    "    except:\n",
    "        return -1 # not found\n",
    "\n",
    "# Enforces <T><R><S> Structure\n",
    "class TRSLogits(LogitsProcessor):\n",
    "    def __init__(self, _tokenizer):\n",
    "        self.tokenizer = _tokenizer\n",
    "        self.t_token = _tokenizer(\"<unused0>\")['input_ids'][1] # <T>\n",
    "        self.r_token = _tokenizer(\"<unused1>\")['input_ids'][1] # <R>\n",
    "        self.s_token = _tokenizer(\"<unused2>\")['input_ids'][1] # <S>\n",
    "        \n",
    "        self.eos_token = _tokenizer(\"<eos>\")['input_ids'][1] # <EOS>\n",
    "        \n",
    "        # self.response_template_token = _tokenizer(response_template)['input_ids'][1]\n",
    "    \n",
    "    def __call__(self, input_ids, scores) -> torch.FloatTensor:\n",
    "        # get the closest token of interest\n",
    "        ids_list = input_ids.tolist()[0]\n",
    "        # print(ids_list)\n",
    "        \n",
    "        t_near_pos = find_largest_index(ids_list, self.t_token)\n",
    "        r_near_pos = find_largest_index(ids_list, self.r_token)\n",
    "        s_near_pos = find_largest_index(ids_list, self.s_token)\n",
    "        \n",
    "        near_pos = max(t_near_pos, r_near_pos, s_near_pos)\n",
    "        \n",
    "        if near_pos == len(ids_list) - 1:\n",
    "            # Just generated start token\n",
    "            # Enforce content generation (no special tokens!)\n",
    "            banned_tokens = [self.t_token, self.r_token, self.s_token, self.eos_token] # No special tokens allowed\n",
    "            # print(f\"#BAN at [{self.tokenizer.batch_decode(input_ids)[0][-10:]}]\")\n",
    "        # New special token enforce\n",
    "        elif near_pos == t_near_pos:\n",
    "            # T - setup\n",
    "            banned_tokens = [self.t_token,               self.s_token, self.eos_token] # R allowed\n",
    "            # print(f\"#T at [{self.tokenizer.batch_decode(input_ids)[0][-10:]}]\")\n",
    "        elif near_pos == r_near_pos:\n",
    "            # R - setup\n",
    "            banned_tokens = [self.t_token, self.r_token,               self.eos_token] # S allowed\n",
    "            # print(f\"#R at [{self.tokenizer.batch_decode(input_ids)[0][-10:]}]\")\n",
    "        elif near_pos == s_near_pos:\n",
    "            # S - setupz\n",
    "            banned_tokens = [              self.r_token, self.s_token,               ] # T, end allowed\n",
    "            # print(f\"#S at [{self.tokenizer.batch_decode(input_ids)[0][-10:]}]\")\n",
    "        else:\n",
    "            print(\"jjifdoasjddio not supposed to happen arghh!\")\n",
    "            raise Exception\n",
    "               \n",
    "        disabled_scores = disable_tokens(scores, banned_tokens)\n",
    "        # print(input_ids)\n",
    "        # print(f\"{scores} -> {disabled_scores}\")\n",
    "        return disabled_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import re\n",
    "\n",
    "# alpaca_prompt = You MUST copy from above!\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Extract the most confident information in the sentence below as much as possible, and express the relationships in RDF Triples that complement the existing RDF triples. Do not use information from common sense.\n",
    "### Existing RDF triples:\n",
    "{}\n",
    "### Input:\n",
    "{}\n",
    "### Response:\n",
    "<unused0>{}\"\"\" # start with <T>\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Triple:\n",
    "    subject: str\n",
    "    predicate: str\n",
    "    object: str\n",
    "    id: int\n",
    "\n",
    "    def __eq__(self, other: \"Triple\"):  # Python type hinting sucks\n",
    "        return self.subject == other.subject and self.predicate == other.predicate and self.object == other.object\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<unused0>{self.subject}<unused1>{self.predicate}<unused2>{self.object}\"\n",
    "\n",
    "\n",
    "def generate_rdf(context: list[Triple], text: str, id: int) -> list[Triple]:\n",
    "    # context pre-processing\n",
    "    if len(context) == 0:\n",
    "        context_str = \"None\"\n",
    "    else:\n",
    "        context_str = \" \".join(str(con) for con in random.sample(context, min(len(context), 10)))\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        [\n",
    "            prompt.format(\n",
    "                context_str,\n",
    "                text,  # input\n",
    "                \"\",  # output - leave this blank for generation!\n",
    "            )\n",
    "        ], return_tensors=\"pt\").to(\"cuda\")\n",
    "    print(f\"Processing \\\"{text[:50]}...{text[-10:]}\\\"\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        \n",
    "        # temperature = 0.9,\n",
    "        # max_new_tokens = max(len(text) + 100, 700), \n",
    "        \n",
    "        logits_processor = LogitsProcessorList([TRSLogits(tokenizer)]),\n",
    "        # num_beams = 3,\n",
    "        # early_stopping = True,\n",
    "        \n",
    "        use_cache=True, # Use cache = false is broken haha, but beam search is broken when not using cache hahahah ;-;\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.batch_decode(outputs)\n",
    "    \n",
    "    response = response[0].replace('\\n', '')\n",
    "    rdf_string = response.split(\"### Response:\")[1]\n",
    "    \n",
    "    print(f\"Done!\")\n",
    "    # print(rdf_string)\n",
    "    # convert rdf string to list\n",
    "    rdfs = []\n",
    "    rdf_string = rdf_string.removeprefix(\"<bos>\").removesuffix(\"<eos>\")\n",
    "    for _triple in rdf_string.split(\"<unused0>\"):\n",
    "        print(_triple)\n",
    "        try:\n",
    "            if _triple == \"\":\n",
    "                continue\n",
    "            split = re.split(\"<unused1>|<unused2>\", _triple)\n",
    "            subject = split[0]\n",
    "            predicate = split[1]\n",
    "            _object = split[2]\n",
    "\n",
    "            new_triple = Triple(subject, predicate, _object, id)\n",
    "\n",
    "            if not (any(con == new_triple for con in context) or any(con == new_triple for con in rdfs)):\n",
    "                rdfs.append(new_triple)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"NON-STANDARD TRIPLE {_triple} ({e})\")\n",
    "            continue\n",
    "    print(\"DONE\")\n",
    "    return rdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f89a6596d9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = []\n",
    "for idx, m_sentence in enumerate(merged_sentences):\n",
    "    print(f\"PROCESSING ({idx + 1}/{len(merged_sentences)})\")\n",
    "    triples += generate_rdf(triples, m_sentence, idx)\n",
    "    # print(triples)\n",
    "\n",
    "triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4aa95ee3c6d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(bgcolor=\"#222222\", font_color=\"white\", notebook=True, directed=True)\n",
    "\n",
    "\n",
    "# Parse rdf_strings\n",
    "\n",
    "def add_triples(rdf: Triple, color: str):\n",
    "    net.add_node(rdf.subject, color=color)\n",
    "    net.add_node(rdf.object, color=color)\n",
    "    # if not any(edge['from'] == rdf.subject and edge['to'] == rdf.object and edge['title'] == rdf.predicate for edge in net.edges): # should be deprecated later\n",
    "    net.add_edge(rdf.subject, rdf.object, title=rdf.predicate, color=color)\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "r = lambda: random.randint(0, 255)\n",
    "net.toggle_physics(True)\n",
    "\n",
    "colors = {}\n",
    "\n",
    "for idx, triple in enumerate(triples):\n",
    "    if triple.id in colors:\n",
    "        color = colors[triple.id]\n",
    "    else:\n",
    "        color = '#%02X%02X%02X' % (r(), r(), r())\n",
    "        colors[triple.id] = color\n",
    "\n",
    "    add_triples(triple, color)\n",
    "    # net.show(f\"{idx}.html\", notebook=False)\n",
    "\n",
    "print(net.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcead4539ba47f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.toggle_physics(True)\n",
    "#save the HTML instead of show the html\n",
    "\n",
    "# from IPython.core.display import display\n",
    "net.show(\"network.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65057ccbbe0d65f",
   "metadata": {},
   "source": [
    "# Dev: check similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d29155fc4b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_names = [n[\"id\"] for n in net.nodes]\n",
    "node_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66457f4b0f9f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydifflib.py\n",
    "from difflib import SequenceMatcher\n",
    "from heapq import nlargest as _nlargest\n",
    "\n",
    "def get_close_matches_indexes(word, possibilities, n=3, cutoff=0.6):\n",
    "    \"\"\"Use SequenceMatcher to return a list of the indexes of the best \n",
    "    \"good enough\" matches. word is a sequence for which close matches \n",
    "    are desired (typically a string).\n",
    "    possibilities is a list of sequences against which to match word\n",
    "    (typically a list of strings).\n",
    "    Optional arg n (default 3) is the maximum number of close matches to\n",
    "    return.  n must be > 0.\n",
    "    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities\n",
    "    that don't score at least that similar to word are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    if not n >  0:\n",
    "        raise ValueError(\"n must be > 0: %r\" % (n,))\n",
    "    if not 0.0 <= cutoff <= 1.0:\n",
    "        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n",
    "    result = []\n",
    "    s = SequenceMatcher()\n",
    "    s.set_seq2(word)\n",
    "    for idx, x in enumerate(possibilities):\n",
    "        s.set_seq1(x)\n",
    "        if s.real_quick_ratio() >= cutoff and \\\n",
    "           s.quick_ratio() >= cutoff and \\\n",
    "           s.ratio() >= cutoff:\n",
    "            result.append((s.ratio(), idx))\n",
    "\n",
    "    # Move the best scorers to head of list\n",
    "    result = _nlargest(n, result)\n",
    "\n",
    "    # Strip scores for the best n matches\n",
    "    return [x for score, x in result]\n",
    "\n",
    "for idx, name in enumerate(node_names):\n",
    "    # nodes_other = [x for x in node_names if x != name]\n",
    "    indexes = get_close_matches_indexes(name, node_names)\n",
    "    indexes.remove(idx)\n",
    "    if indexes == []:\n",
    "        continue\n",
    "    print(f\"{name}({idx}) | {[f'{node_names[i]}({i})' for i in indexes]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b3abac0558b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
