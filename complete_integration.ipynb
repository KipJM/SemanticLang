{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to use\n",
    "Run all and enter data when prompted"
   ],
   "id": "e496af9caf2df527"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T04:37:04.573828Z",
     "start_time": "2024-11-18T04:36:56.794182Z"
    }
   },
   "source": [
    "from torch.cuda import graph\n",
    "\n",
    "# Get documents\n",
    "documents = []\n",
    "\n",
    "for i in range(int(input(\"Enter the number of documents for the AI to understand\"))):\n",
    "    documents.append(input(\"Copy the document here\").replace(\"\\n\\n\", \"\"))\n",
    "\n",
    "documents"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Donkey Kong Country[b] is a 1994 platform game developed by Rare and published by Nintendo for the Super Nintendo Entertainment System (SNES). It is a reboot of Nintendo's Donkey Kong franchise and follows the gorilla Donkey Kong and his nephew Diddy Kong as they set out to recover their stolen banana hoard from the crocodile King K. Rool and his army, the Kremlings. The single-player traverses 40 side-scrolling levels as they jump between platforms and avoid obstacles. They collect items, ride minecarts and animals, defeat enemies and bosses, and find secret bonus stages. In multiplayer modes, two players work cooperatively or race.After developing Nintendo Entertainment System games in the 1980s, Rare, a British studio founded by Tim and Chris Stamper, purchased Silicon Graphics workstations to render 3D models. Nintendo sought a game to compete with Sega's Aladdin (1993) and commissioned Rare to revive the dormant Donkey Kong franchise. Rare assembled 12 developers to work on Donkey Kong Country over 18 months. Donkey Kong Country was inspired by the Super Mario series and was one of the first home console games to feature pre-rendered graphics, achieved through a compression technique that converted 3D models into SNES sprites with little loss of detail. It was the first Donkey Kong game neither produced nor directed by the franchise's creator Shigeru Miyamoto, though he contributed design ideas.Following its announcement at the Consumer Electronics Show in June 1994, Donkey Kong Country was highly anticipated and backed by a major marketing campaign that cost $16 million in America alone. It was released in November 1994 to acclaim; critics hailed its visuals as groundbreaking and praised its gameplay and music. Its quality and design were favourably compared to the Super Mario series. Donkey Kong Country received several year-end accolades and set the record for the fastest-selling video game at the time. With 9.3 million copies sold worldwide, it is the third-bestselling SNES game and the bestselling Donkey Kong game. Following the success, Nintendo purchased a large minority stake in Rare, which became a prominent second-party developer for Nintendo during the late 1990s.\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T04:37:05.607912Z",
     "start_time": "2024-11-18T04:37:05.600676Z"
    }
   },
   "cell_type": "code",
   "source": "graph_gen = None",
   "id": "d87bd9b3458b0270",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T04:39:09.359688Z",
     "start_time": "2024-11-18T04:37:06.026474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from GraphGen.graph_gen_integration import GraphGen\n",
    "# Parse documents\n",
    "triples = []\n",
    "chunks = []\n",
    "\n",
    "if graph_gen is None:\n",
    "    graph_gen = GraphGen([], keyword_reward=1.05)\n",
    "\n",
    "\n",
    "for idx, document in enumerate(documents):\n",
    "    print(f\"==================== Process document {idx+1}/{len(documents)} ====================\")\n",
    "    graph_gen.reset_model(triples, idx)\n",
    "    triples += graph_gen.generate(document)\n",
    "    chunks.append(graph_gen.split_chunks(document))\n",
    "    \n",
    "print(\"ACTUALLY DONE!\")"
   ],
   "id": "50d4b9d3130150ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.11.5: Fast Gemma2 patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 30.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.11.5 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Process document 1/1 ====================\n",
      "PROCESSING (1/13)\n",
      "Phase 1 text chunk \"Donkey Kong Country[b] is a 1994 platform game dev...army, the Kremlings.\"\n",
      "Phase 1 context \"None...None\"\n",
      "---\n",
      "Phase 1 model start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUTOTUNE bmm(16x182x256, 16x256x182)\n",
      "  bmm 0.0133 ms 100.0% \n",
      "  triton_bmm_2 0.0184 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_bmm_3 0.0184 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_bmm_6 0.0184 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_9 0.0195 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_10 0.0195 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_bmm_0 0.0205 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=1, num_warps=2\n",
      "  triton_bmm_5 0.0205 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_13 0.0205 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_14 0.0225 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.5890 seconds and 0.0030 seconds precompiling\n",
      "AUTOTUNE bmm(16x182x182, 16x182x256)\n",
      "  bmm 0.0133 ms 100.0% \n",
      "  triton_bmm_22 0.0174 ms 76.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_bmm_25 0.0205 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_29 0.0205 ms 65.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_bmm_33 0.0215 ms 61.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=4, num_warps=8\n",
      "  triton_bmm_24 0.0225 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=4\n",
      "  triton_bmm_26 0.0236 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=8\n",
      "  triton_bmm_32 0.0236 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=3, num_warps=4\n",
      "  triton_bmm_37 0.0236 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=5, num_warps=8\n",
      "  triton_bmm_34 0.0246 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=False, GROUP_M=8, num_stages=2, num_warps=8\n",
      "SingleProcess AUTOTUNE benchmarking takes 2.2022 seconds and 0.0040 seconds precompiling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>publisher<unused2>Nintendo\n",
      "Donkey_Kong_Country<unused1>starring<unused2>Diddy_Kong\n",
      "Donkey_Kong_Country<unused1>world<unused2>Kongo\n",
      "Donkey_Kong_Country<unused1>character<unused2>King_K._Rool\n",
      "Donkey_Kong_Country<unused1>genre<unused2>Platform_game\n",
      "Donkey_Kong_Country<unused1>developer<unused2>Rare\n",
      "Donkey_Kong_Country<unused1>incident<unused2>Banana_hoard\n",
      "Donkey_Kong_Country<unused1>platform<unused2>Super_Nintendo_Entertainment_System\n",
      "Donkey_Kong_Country<unused1>yearOfPublication<unused2>1994\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (2/13)\n",
      "Phase 1 text chunk \"It is a reboot of Nintendo's Donkey Kong franchise...army, the Kremlings.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_Country<u...unused2>Banana_hoard\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>followedBy<unused2>Donkey_Kong_64\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (3/13)\n",
      "Phase 1 text chunk \"The single-player traverses 40 side-scrolling leve...secret bonus stages.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_Country<u...g<unused2>Diddy_Kong\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>numberOfLevels<unused2>40\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (4/13)\n",
      "Phase 1 text chunk \"They collect items, ride minecarts and animals, de...secret bonus stages.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_Country<u...rOfLevels<unused2>40\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>minigame<unused2>\"Bionico\"\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (5/13)\n",
      "Phase 1 text chunk \"In multiplayer modes, two players work cooperative...nkey Kong franchise.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_Country<u...Entertainment_System\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Rare<unused1>founder<unused2>Tim_Stamp\n",
      "Donkey_Kong_64<unused1>developer<unused2>Rare\n",
      "Rare<unused1>foundedBy<unused2>Chris_Stamp\n",
      "Rare<unused1>location<unused2>Britain\n",
      "Donkey_Kong_Country<unused1>console<unused2>Nintendo_Entertainment_System\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (6/13)\n",
      "Phase 1 text chunk \"Nintendo sought a game to compete with Sega's Alad...nkey Kong franchise.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_Country<u...used2>Donkey_Kong_64\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Aladdin_(1993_video_game)<unused1>publisher<unused2>Sega\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (7/13)\n",
      "Phase 1 text chunk \"Rare assembled 12 developers to work on Donkey Kon...ttle loss of detail.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_Country<u...nused2>Platform_game\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>timeToDevelop<unused2>\"18 months\"\n",
      "Donkey_Kong_Country<unused1>developer<unused2>12_individuals\n",
      "Donkey_Kong_Country<unused1>precededBy<unused2>Super_Mario_Bros.\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (8/13)\n",
      "Phase 1 text chunk \"Donkey Kong Country was inspired by the Super Mari...ttle loss of detail.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_Country<u...me<unused2>\"Bionico\"\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>precededBy<unused2>Super_Mario_Bros.\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (9/13)\n",
      "Phase 1 text chunk \"It was the first Donkey Kong game neither produced...on in America alone.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_64<unused...used2>12_individuals\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>creator<unused2>Shigeru_Miyamoto\n",
      "Donkey_Kong_Country<unused1>announcement<unused2>Consumer_Electronics_Show\n",
      "Donkey_Kong_Country<unused1>region<unused2>\"Worldwide\"\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (10/13)\n",
      "Phase 1 text chunk \"It was the first Donkey Kong game neither produced...on in America alone.\"\n",
      "Phase 1 context \"<unused0>Rare<unused1>foundedB...<unused2>\"18 months\"\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>marketingCost<unused2>\"16 million (American dollars)\"\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (11/13)\n",
      "Phase 1 text chunk \"It was released in November 1994 to acclaim; criti...eo game at the time.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_Country<u...unused2>King_K._Rool\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>visualInspection<unused2>\"Groundbreaking visuals\"\n",
      "Donkey_Kong_Country<unused1>music<unused2>\"Groundbreaking music\"\n",
      "Donkey_Kong_Country<unused1>releaseDate<unused2>\"November 1994\"\n",
      "Donkey_Kong_Country<unused1>comparison<unused2>Super_Mario_Bros.\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (12/13)\n",
      "Phase 1 text chunk \"Donkey Kong Country received several year-end acco...eo game at the time.\"\n",
      "Phase 1 context \"<unused0>Rare<unused1>location...d2>Super_Mario_Bros.\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Donkey_Kong_Country<unused1>award<unused2>\"Best Selling Video Game of 1994\"\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "PROCESSING (13/13)\n",
      "Phase 1 text chunk \"With 9.3 million copies sold worldwide, it is the ...ring the late 1990s.\"\n",
      "Phase 1 context \"<unused0>Donkey_Kong_Country<u...sed2>\"November 1994\"\"\n",
      "---\n",
      "Phase 1 model start\n",
      "Phase 2 model generation done\n",
      "\n",
      "Rare<unused1>location<unused2>London\n",
      "Phase 3 parsing done\n",
      "chunk DONE\n",
      "ACTUALLY DONE!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T04:39:11.979985Z",
     "start_time": "2024-11-18T04:39:11.962638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "with open('triples.smlgraph', 'wb') as outp:\n",
    "    pickle.dump(triples, outp)\n",
    "    \n",
    "triples"
   ],
   "id": "ae023bd615c5aff1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Triple(subject='Donkey_Kong_Country', predicate='publisher', object='Nintendo', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='starring', object='Diddy_Kong', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='world', object='Kongo', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='character', object='King_K._Rool', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='genre', object='Platform_game', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='developer', object='Rare', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='incident', object='Banana_hoard', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='platform', object='Super_Nintendo_Entertainment_System', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='yearOfPublication', object='1994', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='followedBy', object='Donkey_Kong_64', id=1, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='numberOfLevels', object='40', id=2, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='minigame', object='\"Bionico\"', id=3, document_id=0),\n",
       " Triple(subject='Rare', predicate='founder', object='Tim_Stamp', id=4, document_id=0),\n",
       " Triple(subject='Donkey_Kong_64', predicate='developer', object='Rare', id=4, document_id=0),\n",
       " Triple(subject='Rare', predicate='foundedBy', object='Chris_Stamp', id=4, document_id=0),\n",
       " Triple(subject='Rare', predicate='location', object='Britain', id=4, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='console', object='Nintendo_Entertainment_System', id=4, document_id=0),\n",
       " Triple(subject='Aladdin_(1993_video_game)', predicate='publisher', object='Sega', id=5, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='timeToDevelop', object='\"18 months\"', id=6, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='developer', object='12_individuals', id=6, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='precededBy', object='Super_Mario_Bros.', id=6, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='creator', object='Shigeru_Miyamoto', id=8, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='announcement', object='Consumer_Electronics_Show', id=8, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='region', object='\"Worldwide\"', id=8, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='marketingCost', object='\"16 million (American dollars)\"', id=9, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='visualInspection', object='\"Groundbreaking visuals\"', id=10, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='music', object='\"Groundbreaking music\"', id=10, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='releaseDate', object='\"November 1994\"', id=10, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='comparison', object='Super_Mario_Bros.', id=10, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='award', object='\"Best Selling Video Game of 1994\"', id=11, document_id=0),\n",
       " Triple(subject='Rare', predicate='location', object='London', id=12, document_id=0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T04:39:16.309505Z",
     "start_time": "2024-11-18T04:39:16.250821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyvis.network import Network\n",
    "from GraphGen.graph_gen_integration import Triple\n",
    "from GraphGen import graph_gen_integration\n",
    "\n",
    "net = Network(bgcolor=\"#222222\", font_color=\"white\", notebook=True, directed=True)\n",
    "net2 = Network(bgcolor=\"#222222\", font_color=\"white\", notebook=False, directed=True)\n",
    "\n",
    "\n",
    "# Parse rdf_strings\n",
    "\n",
    "def add_triples(rdf: Triple, _color: str):\n",
    "    net.add_node(rdf.subject, color=_color)\n",
    "    net.add_node(rdf.object, color=_color)\n",
    "    # if not any(edge['from'] == rdf.subject and edge['to'] == rdf.object and edge['title'] == rdf.predicate for edge in net.edges): # should be deprecated later\n",
    "    net.add_edge(rdf.subject, rdf.object, title=rdf.predicate, color=_color)\n",
    "    \n",
    "    net2.add_node(rdf.subject, color=_color)\n",
    "    net2.add_node(rdf.object, color=_color)\n",
    "    # if not any(edge['from'] == rdf.subject and edge['to'] == rdf.object and edge['title'] == rdf.predicate for edge in net.edges): # should be deprecated later\n",
    "    net2.add_edge(rdf.subject, rdf.object, title=rdf.predicate, color=_color)\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "r = lambda: random.randint(0, 255)\n",
    "net.toggle_physics(True)\n",
    "\n",
    "colors = {}\n",
    "\n",
    "for idx, triple in enumerate(triples):\n",
    "    if triple.document_id in colors:\n",
    "        document = colors[triple.document_id]\n",
    "        if triple.id in document:\n",
    "            color = document[triple.id]\n",
    "        else:\n",
    "            color = '#%02X%02X%02X' % (r(), r(), r())\n",
    "            colors[triple.document_id][triple.id] = color\n",
    "    else:\n",
    "        colors[triple.document_id] = {} \n",
    "        color = '#%02X%02X%02X' % (r(), r(), r())\n",
    "        colors[triple.document_id][triple.id] = color\n",
    "\n",
    "    add_triples(triple, color)\n",
    "    # net.show(f\"{idx}.html\", notebook=False)\n",
    "net.show(\"graph.html\", notebook=True)\n",
    "# net2.show(\"graph2.html\", notebook=False) # For visualization outside notebook"
   ],
   "id": "4afb31f010c4930b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "graph.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fec9be05650>"
      ],
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"graph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T04:44:31.512444Z",
     "start_time": "2024-11-18T04:44:31.495568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Workaround for now. Backtracking to source\n",
    "def get_chunk(_hex_code):\n",
    "    _hex_code = _hex_code.upper()\n",
    "    print(_hex_code)\n",
    "    possible_docs = []\n",
    "    for _document in colors:\n",
    "        for _chunk in colors[_document]:\n",
    "            if colors[_document][_chunk] == _hex_code:\n",
    "                possible_docs.append([_document, _chunk])\n",
    "                \n",
    "    if len(possible_docs) == 0:\n",
    "        return None\n",
    "    \n",
    "    # print(possible_docs)\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    for doc in possible_docs:\n",
    "       docs.append(graph_gen.split_chunks(graph_gen.preprocess_document(documents[doc[0]]))[doc[1]])\n",
    "    \n",
    "    return docs"
   ],
   "id": "4b0de6f618d2567e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T04:44:32.997610Z",
     "start_time": "2024-11-18T04:44:32.187047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_gen.free_model()\n",
    "del graph_gen"
   ],
   "id": "bfd159f592bd1ffb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T04:44:34.285646Z",
     "start_time": "2024-11-18T04:44:34.045055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch, gc\n",
    "# graph_gen.free_model()\n",
    "# del graph_gen\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "1145bd943aee9491",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T04:44:35.144056Z",
     "start_time": "2024-11-18T04:44:35.131135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('triples.smlgraph', 'rb') as inp:\n",
    "    triples = pickle.load(inp)\n",
    "triples"
   ],
   "id": "89eacb061ad178e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Triple(subject='Donkey_Kong_Country', predicate='publisher', object='Nintendo', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='starring', object='Diddy_Kong', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='world', object='Kongo', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='character', object='King_K._Rool', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='genre', object='Platform_game', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='developer', object='Rare', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='incident', object='Banana_hoard', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='platform', object='Super_Nintendo_Entertainment_System', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='yearOfPublication', object='1994', id=0, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='followedBy', object='Donkey_Kong_64', id=1, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='numberOfLevels', object='40', id=2, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='minigame', object='\"Bionico\"', id=3, document_id=0),\n",
       " Triple(subject='Rare', predicate='founder', object='Tim_Stamp', id=4, document_id=0),\n",
       " Triple(subject='Donkey_Kong_64', predicate='developer', object='Rare', id=4, document_id=0),\n",
       " Triple(subject='Rare', predicate='foundedBy', object='Chris_Stamp', id=4, document_id=0),\n",
       " Triple(subject='Rare', predicate='location', object='Britain', id=4, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='console', object='Nintendo_Entertainment_System', id=4, document_id=0),\n",
       " Triple(subject='Aladdin_(1993_video_game)', predicate='publisher', object='Sega', id=5, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='timeToDevelop', object='\"18 months\"', id=6, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='developer', object='12_individuals', id=6, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='precededBy', object='Super_Mario_Bros.', id=6, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='creator', object='Shigeru_Miyamoto', id=8, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='announcement', object='Consumer_Electronics_Show', id=8, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='region', object='\"Worldwide\"', id=8, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='marketingCost', object='\"16 million (American dollars)\"', id=9, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='visualInspection', object='\"Groundbreaking visuals\"', id=10, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='music', object='\"Groundbreaking music\"', id=10, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='releaseDate', object='\"November 1994\"', id=10, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='comparison', object='Super_Mario_Bros.', id=10, document_id=0),\n",
       " Triple(subject='Donkey_Kong_Country', predicate='award', object='\"Best Selling Video Game of 1994\"', id=11, document_id=0),\n",
       " Triple(subject='Rare', predicate='location', object='London', id=12, document_id=0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T04:44:50.607609Z",
     "start_time": "2024-11-18T04:44:37.730236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from QueryGen.query_gen_integration import QueryGen\n",
    "query_gen = QueryGen(triples)"
   ],
   "id": "a4f0bbf5189f2e1c",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like unsloth/gemma-2-2b-bnb-4bit is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTimeoutError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/connection.py:199\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 199\u001B[0m     sock \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39mcreate_connection(\n\u001B[1;32m    200\u001B[0m         (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dns_host, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport),\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout,\n\u001B[1;32m    202\u001B[0m         source_address\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource_address,\n\u001B[1;32m    203\u001B[0m         socket_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket_options,\n\u001B[1;32m    204\u001B[0m     )\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 85\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     72\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m---> 73\u001B[0m sock\u001B[38;5;241m.\u001B[39mconnect(sa)\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n",
      "\u001B[0;31mTimeoutError\u001B[0m: timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mConnectTimeoutError\u001B[0m                       Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_request(\n\u001B[1;32m    790\u001B[0m     conn,\n\u001B[1;32m    791\u001B[0m     method,\n\u001B[1;32m    792\u001B[0m     url,\n\u001B[1;32m    793\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout_obj,\n\u001B[1;32m    794\u001B[0m     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    795\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    796\u001B[0m     chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[1;32m    797\u001B[0m     retries\u001B[38;5;241m=\u001B[39mretries,\n\u001B[1;32m    798\u001B[0m     response_conn\u001B[38;5;241m=\u001B[39mresponse_conn,\n\u001B[1;32m    799\u001B[0m     preload_content\u001B[38;5;241m=\u001B[39mpreload_content,\n\u001B[1;32m    800\u001B[0m     decode_content\u001B[38;5;241m=\u001B[39mdecode_content,\n\u001B[1;32m    801\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mresponse_kw,\n\u001B[1;32m    802\u001B[0m )\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/connectionpool.py:490\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    489\u001B[0m         new_e \u001B[38;5;241m=\u001B[39m _wrap_proxy_error(new_e, conn\u001B[38;5;241m.\u001B[39mproxy\u001B[38;5;241m.\u001B[39mscheme)\n\u001B[0;32m--> 490\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m new_e\n\u001B[1;32m    492\u001B[0m \u001B[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001B[39;00m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 466\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_conn(conn)\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError) \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/connectionpool.py:1095\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._validate_conn\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_closed:\n\u001B[0;32m-> 1095\u001B[0m     conn\u001B[38;5;241m.\u001B[39mconnect()\n\u001B[1;32m   1097\u001B[0m \u001B[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/connection.py:693\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    692\u001B[0m sock: socket\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m|\u001B[39m ssl\u001B[38;5;241m.\u001B[39mSSLSocket\n\u001B[0;32m--> 693\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_conn()\n\u001B[1;32m    694\u001B[0m server_hostname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/connection.py:208\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SocketTimeout \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 208\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ConnectTimeoutError(\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    210\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m timed out. (connect timeout=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    211\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mConnectTimeoutError\u001B[0m: (<urllib3.connection.HTTPSConnection object at 0x7fed0684f310>, 'Connection to huggingface.co timed out. (connect timeout=10)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m conn\u001B[38;5;241m.\u001B[39murlopen(\n\u001B[1;32m    668\u001B[0m         method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    669\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m    670\u001B[0m         body\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mbody,\n\u001B[1;32m    671\u001B[0m         headers\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m    672\u001B[0m         redirect\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    673\u001B[0m         assert_same_host\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    674\u001B[0m         preload_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    675\u001B[0m         decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    676\u001B[0m         retries\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries,\n\u001B[1;32m    677\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m    678\u001B[0m         chunked\u001B[38;5;241m=\u001B[39mchunked,\n\u001B[1;32m    679\u001B[0m     )\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/connectionpool.py:843\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    841\u001B[0m     new_e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, new_e)\n\u001B[0;32m--> 843\u001B[0m retries \u001B[38;5;241m=\u001B[39m retries\u001B[38;5;241m.\u001B[39mincrement(\n\u001B[1;32m    844\u001B[0m     method, url, error\u001B[38;5;241m=\u001B[39mnew_e, _pool\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m, _stacktrace\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39mexc_info()[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    845\u001B[0m )\n\u001B[1;32m    846\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/urllib3/util/retry.py:519\u001B[0m, in \u001B[0;36mRetry.increment\u001B[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[1;32m    518\u001B[0m     reason \u001B[38;5;241m=\u001B[39m error \u001B[38;5;129;01mor\u001B[39;00m ResponseError(cause)\n\u001B[0;32m--> 519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxRetryError(_pool, url, reason) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mreason\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    521\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncremented Retry for (url=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, url, new_retry)\n",
      "\u001B[0;31mMaxRetryError\u001B[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/gemma-2-2b-bnb-4bit/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fed0684f310>, 'Connection to huggingface.co timed out. (connect timeout=10)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mConnectTimeout\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1376\u001B[0m, in \u001B[0;36m_get_metadata_or_catch_error\u001B[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001B[0m\n\u001B[1;32m   1375\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1376\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m get_hf_file_metadata(\n\u001B[1;32m   1377\u001B[0m         url\u001B[38;5;241m=\u001B[39murl, proxies\u001B[38;5;241m=\u001B[39mproxies, timeout\u001B[38;5;241m=\u001B[39metag_timeout, headers\u001B[38;5;241m=\u001B[39mheaders, token\u001B[38;5;241m=\u001B[39mtoken\n\u001B[1;32m   1378\u001B[0m     )\n\u001B[1;32m   1379\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1296\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001B[0m\n\u001B[1;32m   1295\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[0;32m-> 1296\u001B[0m r \u001B[38;5;241m=\u001B[39m _request_wrapper(\n\u001B[1;32m   1297\u001B[0m     method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHEAD\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1298\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m   1299\u001B[0m     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m   1300\u001B[0m     allow_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   1301\u001B[0m     follow_relative_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m   1302\u001B[0m     proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m   1303\u001B[0m     timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[1;32m   1304\u001B[0m )\n\u001B[1;32m   1305\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:277\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[0;32m--> 277\u001B[0m     response \u001B[38;5;241m=\u001B[39m _request_wrapper(\n\u001B[1;32m    278\u001B[0m         method\u001B[38;5;241m=\u001B[39mmethod,\n\u001B[1;32m    279\u001B[0m         url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m    280\u001B[0m         follow_relative_redirects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    282\u001B[0m     )\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:300\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[0;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001B[39;00m\n\u001B[0;32m--> 300\u001B[0m response \u001B[38;5;241m=\u001B[39m get_session()\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[1;32m    301\u001B[0m hf_raise_for_status(response)\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:93\u001B[0m, in \u001B[0;36mUniqueRequestIdAdapter.send\u001B[0;34m(self, request, *args, **kwargs)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/requests/adapters.py:688\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    687\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e\u001B[38;5;241m.\u001B[39mreason, NewConnectionError):\n\u001B[0;32m--> 688\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ConnectTimeout(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m    690\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e\u001B[38;5;241m.\u001B[39mreason, ResponseError):\n",
      "\u001B[0;31mConnectTimeout\u001B[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/gemma-2-2b-bnb-4bit/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fed0684f310>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 04d82073-5780-454b-95c6-a055beb91272)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mLocalEntryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 403\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m hf_hub_download(\n\u001B[1;32m    404\u001B[0m         path_or_repo_id,\n\u001B[1;32m    405\u001B[0m         filename,\n\u001B[1;32m    406\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(subfolder) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m subfolder,\n\u001B[1;32m    407\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[1;32m    408\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    409\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    410\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    411\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    412\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    413\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    414\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    415\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    416\u001B[0m     )\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[0;32m--> 114\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:862\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001B[0m\n\u001B[1;32m    861\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 862\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _hf_hub_download_to_cache_dir(\n\u001B[1;32m    863\u001B[0m         \u001B[38;5;66;03m# Destination\u001B[39;00m\n\u001B[1;32m    864\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    865\u001B[0m         \u001B[38;5;66;03m# File info\u001B[39;00m\n\u001B[1;32m    866\u001B[0m         repo_id\u001B[38;5;241m=\u001B[39mrepo_id,\n\u001B[1;32m    867\u001B[0m         filename\u001B[38;5;241m=\u001B[39mfilename,\n\u001B[1;32m    868\u001B[0m         repo_type\u001B[38;5;241m=\u001B[39mrepo_type,\n\u001B[1;32m    869\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    870\u001B[0m         \u001B[38;5;66;03m# HTTP info\u001B[39;00m\n\u001B[1;32m    871\u001B[0m         endpoint\u001B[38;5;241m=\u001B[39mendpoint,\n\u001B[1;32m    872\u001B[0m         etag_timeout\u001B[38;5;241m=\u001B[39metag_timeout,\n\u001B[1;32m    873\u001B[0m         headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    874\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    875\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    876\u001B[0m         \u001B[38;5;66;03m# Additional options\u001B[39;00m\n\u001B[1;32m    877\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    878\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    879\u001B[0m     )\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:969\u001B[0m, in \u001B[0;36m_hf_hub_download_to_cache_dir\u001B[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001B[0m\n\u001B[1;32m    968\u001B[0m     \u001B[38;5;66;03m# Otherwise, raise appropriate error\u001B[39;00m\n\u001B[0;32m--> 969\u001B[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001B[1;32m    971\u001B[0m \u001B[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1487\u001B[0m, in \u001B[0;36m_raise_on_head_call_error\u001B[0;34m(head_call_error, force_download, local_files_only)\u001B[0m\n\u001B[1;32m   1485\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1486\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n\u001B[0;32m-> 1487\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m LocalEntryNotFoundError(\n\u001B[1;32m   1488\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is on.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1491\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhead_call_error\u001B[39;00m\n",
      "\u001B[0;31mLocalEntryNotFoundError\u001B[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mQueryGen\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mquery_gen_integration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m QueryGen\n\u001B[0;32m----> 2\u001B[0m query_gen \u001B[38;5;241m=\u001B[39m QueryGen(triples)\n",
      "File \u001B[0;32m~/siton-tmp/SemanticLang/QueryGen/query_gen_integration.py:191\u001B[0m, in \u001B[0;36mQueryGen.__init__\u001B[0;34m(self, existing_rdfs)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 191\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetup_model()\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m existing_rdfs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(existing_rdfs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mappend_rdfs(existing_rdfs)\n",
      "File \u001B[0;32m~/siton-tmp/SemanticLang/QueryGen/query_gen_integration.py:258\u001B[0m, in \u001B[0;36mQueryGen.setup_model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msetup_model\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    257\u001B[0m     \u001B[38;5;66;03m# setup model helper function\u001B[39;00m\n\u001B[0;32m--> 258\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m FastLanguageModel\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    259\u001B[0m         model_name\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(\u001B[38;5;18m__file__\u001B[39m),\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquery_2b_treefix_2000step\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m    260\u001B[0m         max_seq_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2048\u001B[39m,\n\u001B[1;32m    261\u001B[0m         dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    262\u001B[0m         load_in_4bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    263\u001B[0m     )\n\u001B[1;32m    264\u001B[0m     FastLanguageModel\u001B[38;5;241m.\u001B[39mfor_inference(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel)\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/unsloth/models/loader.py:265\u001B[0m, in \u001B[0;36mFastLanguageModel.from_pretrained\u001B[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_peft:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;66;03m# Check base model again for PEFT\u001B[39;00m\n\u001B[1;32m    264\u001B[0m     model_name \u001B[38;5;241m=\u001B[39m get_model_name(peft_config\u001B[38;5;241m.\u001B[39mbase_model_name_or_path, load_in_4bit)\n\u001B[0;32m--> 265\u001B[0m     model_config \u001B[38;5;241m=\u001B[39m AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m    266\u001B[0m         model_name,\n\u001B[1;32m    267\u001B[0m         token \u001B[38;5;241m=\u001B[39m token,\n\u001B[1;32m    268\u001B[0m         revision \u001B[38;5;241m=\u001B[39m revision,\n\u001B[1;32m    269\u001B[0m         trust_remote_code \u001B[38;5;241m=\u001B[39m trust_remote_code,\n\u001B[1;32m    270\u001B[0m     )\n\u001B[1;32m    271\u001B[0m \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m was_disabled: enable_progress_bars()\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py:1017\u001B[0m, in \u001B[0;36mAutoConfig.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m   1014\u001B[0m trust_remote_code \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrust_remote_code\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m   1015\u001B[0m code_revision \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcode_revision\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m-> 1017\u001B[0m config_dict, unused_kwargs \u001B[38;5;241m=\u001B[39m PretrainedConfig\u001B[38;5;241m.\u001B[39mget_config_dict(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1018\u001B[0m has_remote_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAutoConfig\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto_map\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1019\u001B[0m has_local_code \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m config_dict \u001B[38;5;129;01mand\u001B[39;00m config_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_type\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;129;01min\u001B[39;00m CONFIG_MAPPING\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/transformers/configuration_utils.py:574\u001B[0m, in \u001B[0;36mPretrainedConfig.get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    572\u001B[0m original_kwargs \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(kwargs)\n\u001B[1;32m    573\u001B[0m \u001B[38;5;66;03m# Get config dict associated with the base config file\u001B[39;00m\n\u001B[0;32m--> 574\u001B[0m config_dict, kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_get_config_dict(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    575\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    576\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {}, kwargs\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/transformers/configuration_utils.py:633\u001B[0m, in \u001B[0;36mPretrainedConfig._get_config_dict\u001B[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001B[0m\n\u001B[1;32m    629\u001B[0m configuration_file \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_configuration_file\u001B[39m\u001B[38;5;124m\"\u001B[39m, CONFIG_NAME) \u001B[38;5;28;01mif\u001B[39;00m gguf_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m gguf_file\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001B[39;00m\n\u001B[0;32m--> 633\u001B[0m     resolved_config_file \u001B[38;5;241m=\u001B[39m cached_file(\n\u001B[1;32m    634\u001B[0m         pretrained_model_name_or_path,\n\u001B[1;32m    635\u001B[0m         configuration_file,\n\u001B[1;32m    636\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39mcache_dir,\n\u001B[1;32m    637\u001B[0m         force_download\u001B[38;5;241m=\u001B[39mforce_download,\n\u001B[1;32m    638\u001B[0m         proxies\u001B[38;5;241m=\u001B[39mproxies,\n\u001B[1;32m    639\u001B[0m         resume_download\u001B[38;5;241m=\u001B[39mresume_download,\n\u001B[1;32m    640\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39mlocal_files_only,\n\u001B[1;32m    641\u001B[0m         token\u001B[38;5;241m=\u001B[39mtoken,\n\u001B[1;32m    642\u001B[0m         user_agent\u001B[38;5;241m=\u001B[39muser_agent,\n\u001B[1;32m    643\u001B[0m         revision\u001B[38;5;241m=\u001B[39mrevision,\n\u001B[1;32m    644\u001B[0m         subfolder\u001B[38;5;241m=\u001B[39msubfolder,\n\u001B[1;32m    645\u001B[0m         _commit_hash\u001B[38;5;241m=\u001B[39mcommit_hash,\n\u001B[1;32m    646\u001B[0m     )\n\u001B[1;32m    647\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    648\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, kwargs\n",
      "File \u001B[0;32m/opt/conda/envs/semanticlang_env/lib/python3.11/site-packages/transformers/utils/hub.py:446\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    441\u001B[0m         resolved_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    442\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_missing_entries\n\u001B[1;32m    443\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_connection_errors\n\u001B[1;32m    444\u001B[0m     ):\n\u001B[1;32m    445\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m resolved_file\n\u001B[0;32m--> 446\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    447\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe couldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt connect to \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m to load this file, couldn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find it in the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    448\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m cached files and it looks like \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not the path to a directory containing a file named\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    449\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfull_filename\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    450\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    451\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001B[0;31mOSError\u001B[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like unsloth/gemma-2-2b-bnb-4bit is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T02:00:19.535330Z",
     "start_time": "2024-11-14T01:59:50.677224Z"
    }
   },
   "cell_type": "code",
   "source": "question = input(\"Enter your question:\")",
   "id": "f6dcc49bbe973a6c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T02:00:25.940778Z",
     "start_time": "2024-11-14T02:00:20.907487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# question = input(\"Enter your question: \")\n",
    "query = query_gen.generate(question)\n",
    "# query\n",
    "print(query)"
   ],
   "id": "57af6fc682716e52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "A\n",
      "NOT STARTED\n",
      "SELECT DISTINCT ?uri WHERE {?uri <purpose> <Federal_Communications_Commission>  . }\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T06:56:26.234858Z",
     "start_time": "2024-10-31T06:56:26.231368Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d70cd388b9e46775",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T06:56:26.921726Z",
     "start_time": "2024-10-31T06:56:26.890443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from rdflib import Graph, Literal, RDF, URIRef\n",
    "g = Graph()\n",
    "for triple in triples:\n",
    "    g.add((URIRef(triple.subject), URIRef(triple.predicate), URIRef(triple.object)))"
   ],
   "id": "702c31abea511a08",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"Nationwide\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Scholars' Bowl\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Scholars' Bowl\", \"Academic Bowl\", \"Academic Team\", \"Academic Challenge\", \"Scholastic Bowl\", \"Primary School Quiz Bowl\", \"Middle School Quiz Bowl\", \"High School Quiz Bowl\", \"University Quiz Bowl\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Nationwide\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Unlimited\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Unlimited\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Single-elimination tournament\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"4 per team\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Buzzer\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"1953-09-28\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"1970-04-12\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"The College Quiz Bowl\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"College Bowl\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"1959-09-28\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"1948-09-28\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Delaware County, Pennsylvania\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Scott Paper Company\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"1959-09-28\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"1961-09-28\" does not look like a valid URI, trying to serialize this will break.\n",
      "\"Nationwide\" does not look like a valid URI, trying to serialize this will break.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T06:56:28.081571Z",
     "start_time": "2024-10-31T06:56:28.068903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the query to the graph and iterate through results\n",
    "r_r = []\n",
    "for r in g.query(query):\n",
    "    print(r[0])\n",
    "    r_r.append(str(r[0]))\n",
    "r_r"
   ],
   "id": "e1cab88b4345216",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T06:57:21.179242Z",
     "start_time": "2024-10-31T06:57:21.150928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_gen.free_model()\n",
    "del query_gen\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "93c4cf07596e9deb",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mquery_gen\u001B[49m\u001B[38;5;241m.\u001B[39mfree_model()\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m query_gen\n\u001B[1;32m      3\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'query_gen' is not defined"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T06:56:54.611228Z",
     "start_time": "2024-10-31T06:56:32.165290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from AnswerGen.answer_gen_integration import AnswerGen\n",
    "answer_gen = AnswerGen()\n",
    "\n",
    "answer = answer_gen.generate(question, str(r_r))\n",
    "answer"
   ],
   "id": "5f173da7b79fb783",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.8: Fast Gemma2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 2060. Max memory: 6.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.0+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.27.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The quizbowl is the sport of the quiz bowl.<eos>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T06:57:28.858522Z",
     "start_time": "2024-10-31T06:57:27.678314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer_gen.free_model()\n",
    "del answer_gen\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "d0f29ffc59d74768",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T06:57:06.840238Z",
     "start_time": "2024-10-31T06:57:06.836163Z"
    }
   },
   "cell_type": "code",
   "source": "print(answer.replace(\"<eos>\", \"\"))",
   "id": "9f75b88ac1a14d59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quizbowl is the sport of the quiz bowl.\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "graph_gen = GraphGen([], keyword_reward=1.1)\n",
    "graph_gen.reset_model([], 0)"
   ],
   "id": "757162f971743fab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T00:53:06.026662Z",
     "start_time": "2024-11-13T00:53:03.509774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CHUNK LOOKUP\n",
    "hex_code = input(\"Enter hex code for chunk lookup\")\n",
    "chunks = get_chunk(hex_code)\n",
    "if chunks is None:\n",
    "    print(\"No related chunk found. Try again\")\n",
    "elif len(chunks) == 1:\n",
    "    print(\"CHUNK: \")\n",
    "    print(chunks[0])\n",
    "elif len(chunks) > 1:\n",
    "    print(f\"{len(chunks)} chunks somehow got assigned the same color. Find them below\")\n",
    "    for chunk in chunks:\n",
    "        print(chunk)"
   ],
   "id": "e4371cc8d0178c57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#D55923\n",
      "CHUNK: \n",
      "Will Wood is an American musician, singer-songwriter, and comedian. [4][5] Wood has released four studio albums; Everything Is a Lot (2015), SELF-iSH (2016), The Normal Album (2020),[6] and \"In case I make it,\" (2022). The former two were released as Will Wood and the Tapeworms, Wood's prior band name. He has additionally released two live albums and a soundtrack.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T07:00:46.783286Z",
     "start_time": "2024-10-31T07:00:46.776652Z"
    }
   },
   "cell_type": "code",
   "source": "colors",
   "id": "9f84fc2829b6bfaa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: '#2B6E1E', 1: '#A29881', 2: '#74686B'},\n",
       " 1: {0: '#355423', 1: '#752F2E', 2: '#A19E54', 3: '#B0681C', 4: '#B5BCC9'}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bb99b79a91c1a71"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
