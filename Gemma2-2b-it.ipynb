{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-12T01:13:55.818964Z",
     "start_time": "2024-11-12T01:12:31.167477Z"
    }
   },
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "    \"unsloth/gemma-2-2b-bnb-4bit\",             # New small Gemma model!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"gemma-2-9b\",\n",
    "            max_seq_length=2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True,\n",
    "        )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: If you want to finetune Gemma 2, install flash-attn to make it faster!\n",
      "To install flash-attn, do the below:\n",
      "\n",
      "pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n",
      "==((====))==  Unsloth 2024.11.5: Fast Gemma2 patching. Transformers = 4.46.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 30.0 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.1. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T12:42:42.680845Z",
     "start_time": "2024-10-31T12:42:40.364141Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6ef1e5e0635b808f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T01:25:09.513385Z",
     "start_time": "2024-11-12T01:25:02.932906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    \"\"\"This sentence is false!\"\"\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 500, repetition_penalty=1.5, temperature = 1)"
   ],
   "id": "881a5da7793cf63e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>This sentence is false!\n",
      "\n",
      "[User 0001]\n",
      "\n",
      "I'm not sure if this has been posted before, but I just found it on the internet. It was a link to an article about how people are so used of seeing \"this\" and that they don;t even notice when there isn's one in front of them anymore...\n",
      "Anyway here goes:\n",
      "\n",
      "\n",
      "<blockquote><b><i>The following statement contains two errors</i></b></blockquote>\n",
      "\n",
      "\n",
      "\n",
      "(If you can find both mistakes then post your answer below)<eos>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "83b6e36b36c1de4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19943f3202ba28f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
